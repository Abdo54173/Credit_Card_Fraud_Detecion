# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mXjkt7-PReIA_mQLmPobfVyRqaD2hOlz
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df =pd.read_csv('/content/creditcard.csv')

df.duplicated().sum()

df.head()

df.info()

print(f'Shape of the dataset: {df.shape}')  #number of rows and columns
print(f'Class Categories: {df.Class.unique()}')  #our categories
print(f'number of records of 0: {(df.Class == 0).sum()}')  #number of records with class 0
print(f'number of records of 1: {(df.Class==1).sum()}')  #number of records with class 1

sns.countplot(x='Class', data=df)

x =df.corr()['Class'][:30].sort_values(ascending=True)
x

x.plot.bar(figsize=(15, 4), title="Correlation between features and Class", grid=True)

y =df.corr()['Class']

df2 =df.copy()
for i in df.columns:
  if abs(y[i]) < 0.13:
    df2.drop(columns=[i] ,inplace=True)

df2.head()

from imblearn.under_sampling import RandomUnderSampler


x = df2.drop('Class', axis=1)
y = df2['Class']

rus = RandomUnderSampler(random_state=42)
x_resampled, y_resampled = rus.fit_resample(x, y)

downsampled_df = pd.concat([pd.DataFrame(x_resampled, columns=x.columns),
                            pd.DataFrame(y_resampled, columns=['Class'])], axis=1)

downsampled_df.head()

print(y_resampled.value_counts())

sns.countplot(x='Class', data=downsampled_df)

sns.scatterplot(x='V11', y='V17', hue='Class', data=df2)

import warnings
warnings.filterwarnings('ignore')

sns.pairplot(downsampled_df, hue='Class')

"""### Removing Outliers From Class 0 Using IQR Method"""

class_0 = downsampled_df[downsampled_df['Class'] == 0]

numerical_columns = class_0.select_dtypes(include=[np.number]).columns.drop('Class')

Q1 = class_0[numerical_columns].quantile(0.25)
Q3 = class_0[numerical_columns].quantile(0.75)
IQR = Q3 - Q1

filtered_entries = ~((class_0[numerical_columns] < (Q1 - 1.5 * IQR)) |
                     (class_0[numerical_columns] > (Q3 + 1.5 * IQR))).any(axis=1)

class_0_cleaned = class_0[filtered_entries]

df_cleaned = pd.concat([class_0_cleaned, downsampled_df[downsampled_df['Class'] == 1]], ignore_index=True)

print(df_cleaned['Class'].value_counts())

plt.figure(figsize=(10, 6))
plt.scatter(df_cleaned[df_cleaned['Class'] == 0]['V11'],
            df_cleaned[df_cleaned['Class'] == 0]['V17'],
            label='0', color='blue', s=20, alpha=0.5, edgecolor='white')

plt.scatter(df_cleaned[df_cleaned['Class'] == 1]['V11'],
            df_cleaned[df_cleaned['Class'] == 1]['V17'],
            label='1', color='orange', s=20, alpha=0.7, edgecolor='white')

plt.xlabel('V11')
plt.ylabel('V17')
plt.legend(title='Class')
plt.title('Scatter plot of V11 vs V17 after IQR Cleaning for Class 0')
plt.show()

"""### **Data Splitting**"""

from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.metrics import classification_report, confusion_matrix

X = df2.drop(columns='Class')
y = df2['Class']

X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

train_data = X_train_orig.copy()
train_data['Class'] = y_train_orig

majority_class = train_data[train_data['Class'] == 0]
minority_class = train_data[train_data['Class'] == 1]

majority_downsampled = resample(majority_class,
                                replace=False,
                                n_samples=len(minority_class),
                                random_state=42)

downsampled_data = pd.concat([majority_downsampled, minority_class])

X_train_downsampled = downsampled_data.drop(columns='Class')
y_train_downsampled = downsampled_data['Class']

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


model = LogisticRegression()
model.fit(X_train, y_train)


y_pred = model.predict(X_test)


print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.tree import DecisionTreeClassifier

# Train
dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_model.fit(X_train, y_train)

# Predictions
y_pred_dt = dt_model.predict(X_test)

# Evaluation
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))